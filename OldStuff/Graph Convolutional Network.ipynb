{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata['h']\n",
    "            return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = GCNLayer(1433, 16)\n",
    "        self.layer2 = GCNLayer(16, 7)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = self.layer2(g, x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/claude/.dgl/cora.zip from https://data.dgl.ai/dataset/cora_raw.zip...\n",
      "Extracting file to /home/claude/.dgl/cora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claude/.virtualenvs/gnn/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/claude/.virtualenvs/gnn/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9218 | Test Acc 0.3600 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.7895 | Test Acc 0.4140 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.6642 | Test Acc 0.4380 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.5616 | Test Acc 0.4750 | Time(s) 0.0482\n",
      "Epoch 00004 | Loss 1.4745 | Test Acc 0.4890 | Time(s) 0.0515\n",
      "Epoch 00005 | Loss 1.3896 | Test Acc 0.5050 | Time(s) 0.0495\n",
      "Epoch 00006 | Loss 1.3057 | Test Acc 0.5150 | Time(s) 0.0514\n",
      "Epoch 00007 | Loss 1.2258 | Test Acc 0.5130 | Time(s) 0.0497\n",
      "Epoch 00008 | Loss 1.1510 | Test Acc 0.5220 | Time(s) 0.0491\n",
      "Epoch 00009 | Loss 1.0819 | Test Acc 0.5430 | Time(s) 0.0503\n",
      "Epoch 00010 | Loss 1.0161 | Test Acc 0.5680 | Time(s) 0.0509\n",
      "Epoch 00011 | Loss 0.9525 | Test Acc 0.5970 | Time(s) 0.0526\n",
      "Epoch 00012 | Loss 0.8912 | Test Acc 0.6250 | Time(s) 0.0524\n",
      "Epoch 00013 | Loss 0.8330 | Test Acc 0.6410 | Time(s) 0.0535\n",
      "Epoch 00014 | Loss 0.7782 | Test Acc 0.6670 | Time(s) 0.0545\n",
      "Epoch 00015 | Loss 0.7268 | Test Acc 0.6860 | Time(s) 0.0546\n",
      "Epoch 00016 | Loss 0.6792 | Test Acc 0.7010 | Time(s) 0.0546\n",
      "Epoch 00017 | Loss 0.6348 | Test Acc 0.7130 | Time(s) 0.0554\n",
      "Epoch 00018 | Loss 0.5938 | Test Acc 0.7360 | Time(s) 0.0560\n",
      "Epoch 00019 | Loss 0.5567 | Test Acc 0.7510 | Time(s) 0.0560\n",
      "Epoch 00020 | Loss 0.5234 | Test Acc 0.7580 | Time(s) 0.0553\n",
      "Epoch 00021 | Loss 0.4933 | Test Acc 0.7640 | Time(s) 0.0546\n",
      "Epoch 00022 | Loss 0.4650 | Test Acc 0.7710 | Time(s) 0.0540\n",
      "Epoch 00023 | Loss 0.4382 | Test Acc 0.7760 | Time(s) 0.0544\n",
      "Epoch 00024 | Loss 0.4130 | Test Acc 0.7780 | Time(s) 0.0544\n",
      "Epoch 00025 | Loss 0.3898 | Test Acc 0.7830 | Time(s) 0.0539\n",
      "Epoch 00026 | Loss 0.3683 | Test Acc 0.7790 | Time(s) 0.0535\n",
      "Epoch 00027 | Loss 0.3484 | Test Acc 0.7770 | Time(s) 0.0534\n",
      "Epoch 00028 | Loss 0.3299 | Test Acc 0.7790 | Time(s) 0.0530\n",
      "Epoch 00029 | Loss 0.3128 | Test Acc 0.7840 | Time(s) 0.0537\n",
      "Epoch 00030 | Loss 0.2968 | Test Acc 0.7850 | Time(s) 0.0540\n",
      "Epoch 00031 | Loss 0.2817 | Test Acc 0.7890 | Time(s) 0.0539\n",
      "Epoch 00032 | Loss 0.2673 | Test Acc 0.7910 | Time(s) 0.0542\n",
      "Epoch 00033 | Loss 0.2536 | Test Acc 0.7900 | Time(s) 0.0538\n",
      "Epoch 00034 | Loss 0.2409 | Test Acc 0.7880 | Time(s) 0.0540\n",
      "Epoch 00035 | Loss 0.2292 | Test Acc 0.7900 | Time(s) 0.0536\n",
      "Epoch 00036 | Loss 0.2181 | Test Acc 0.7880 | Time(s) 0.0533\n",
      "Epoch 00037 | Loss 0.2073 | Test Acc 0.7880 | Time(s) 0.0531\n",
      "Epoch 00038 | Loss 0.1971 | Test Acc 0.7880 | Time(s) 0.0531\n",
      "Epoch 00039 | Loss 0.1878 | Test Acc 0.7920 | Time(s) 0.0531\n",
      "Epoch 00040 | Loss 0.1791 | Test Acc 0.7910 | Time(s) 0.0530\n",
      "Epoch 00041 | Loss 0.1708 | Test Acc 0.7900 | Time(s) 0.0532\n",
      "Epoch 00042 | Loss 0.1630 | Test Acc 0.7900 | Time(s) 0.0530\n",
      "Epoch 00043 | Loss 0.1557 | Test Acc 0.7930 | Time(s) 0.0531\n",
      "Epoch 00044 | Loss 0.1486 | Test Acc 0.7940 | Time(s) 0.0530\n",
      "Epoch 00045 | Loss 0.1421 | Test Acc 0.7930 | Time(s) 0.0531\n",
      "Epoch 00046 | Loss 0.1360 | Test Acc 0.7930 | Time(s) 0.0528\n",
      "Epoch 00047 | Loss 0.1302 | Test Acc 0.7940 | Time(s) 0.0526\n",
      "Epoch 00048 | Loss 0.1248 | Test Acc 0.7940 | Time(s) 0.0527\n",
      "Epoch 00049 | Loss 0.1197 | Test Acc 0.7950 | Time(s) 0.0526\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "for epoch in range(50):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
